{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StickMonkey615/JHCSMod4/blob/main/ANN%20v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRn73TIW7gn9"
      },
      "source": [
        "# Module 4 Guidance\n",
        "\n",
        "This notebook is a template for module 4b and 4c, which will be tested in Google Colab, your code needs to run there.\n",
        "The structure has been provided to improve consistency and make it easier for markers to understand your code but still give students the flexibility to be creative.  You need to populate the required functions to solve this problem.  All dependencies should be documented in the next cell.\n",
        "\n",
        "You can:\n",
        "    add further cells or text blocks to extend or further explain your solution\n",
        "    add further functions\n",
        "\n",
        "Dont:\n",
        "    rename functions\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "I5Pj_LPoJcrT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZxOsuHxz7goC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd6e1c0-4f58-4694-9b81-089b357d26dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.1.3-py3-none-any.whl (135 kB)\n",
            "\u001b[K     |████████████████████████████████| 135 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (7.9.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.21.6)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 39.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.0.10)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.49.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.9.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.1)\n",
            "Installing collected packages: jedi, kt-legacy, keras-tuner\n",
            "Successfully installed jedi-0.18.1 keras-tuner-1.1.3 kt-legacy-1.0.4\n"
          ]
        }
      ],
      "source": [
        "# Fixed dependencies - do not remove or change.\n",
        "import pytest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/gdrive/')\n",
        "# Import your dependencies\n",
        "!pip install --upgrade xlrd > 1.2.0\n",
        "import xlrd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "!pip install keras-tuner --upgrade\n",
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1m_gmKQP7goE"
      },
      "outputs": [],
      "source": [
        "# Import data\n",
        "\n",
        "def import_local_data(file_path):\n",
        "    \"\"\"This function needs to import the data file into collab and return a pandas dataframe\n",
        "    \"\"\"\n",
        "    raw_df = pd.read_excel(file_path)\n",
        "    return raw_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cIljHljB7goF"
      },
      "outputs": [],
      "source": [
        "local_file_path = \"breast-cancer.xls\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LCu51H5Z7goF"
      },
      "outputs": [],
      "source": [
        "# Dont change\n",
        "raw_data = import_local_data(local_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9WDYKUP7goG"
      },
      "source": [
        "### Conduct exploratory data analysis and explain your key findings - Examine the data, explain its key features and what they look like.  Highlight any fields that are anomalous."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the different dataframe column headings\n",
        "print(raw_data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkjdfYHGiz7a",
        "outputId": "e503fdea-5757-4b31-a3ac-11f1079e6a94"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig',\n",
            "       'breast', 'breast-quad', 'irradiat', 'Class'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine data types for each column\n",
        "for i in range(0, len(raw_data.columns)):\n",
        "    print(type(raw_data.values[1][i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKqjAI-XigbI",
        "outputId": "95851474-1531-4352-fd19-fcf708d85aa2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'int'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at the range of values for each field\n",
        "from collections import Counter\n",
        "rng_vals=[]\n",
        "for i in range(0,len(raw_data.columns)):\n",
        "    rng_vals.append(Counter(raw_data.iloc[:,i].values))\n",
        "    print(f\"{raw_data.columns[i]}: {rng_vals[i]}\")\n",
        "del rng_vals, i"
      ],
      "metadata": {
        "id": "-lQUCdTe36Dp",
        "outputId": "ba164b35-3d0b-444f-cbd0-f19a8b8b0332",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age: Counter({'50-59': 96, '40-49': 90, '60-69': 57, '30-39': 36, '70-79': 6, '20-29': 1})\n",
            "menopause: Counter({'premeno': 150, 'ge40': 129, 'lt40': 7})\n",
            "tumor-size: Counter({'30-34': 60, '25-29': 54, '20-24': 50, '15-19': 30, datetime.datetime(2014, 10, 1, 0, 0): 28, '40-44': 22, '35-39': 19, '0-4': 8, '50-54': 8, datetime.datetime(2019, 9, 5, 0, 0): 4, '45-49': 3})\n",
            "inv-nodes: Counter({'0-2': 213, datetime.datetime(2019, 5, 3, 0, 0): 36, datetime.datetime(2019, 8, 6, 0, 0): 17, datetime.datetime(2019, 11, 9, 0, 0): 10, '15-17': 6, datetime.datetime(2014, 12, 1, 0, 0): 3, '24-26': 1})\n",
            "node-caps: Counter({'no': 222, 'yes': 56, '?': 8})\n",
            "deg-malig: Counter({2: 130, 3: 85, 1: 71})\n",
            "breast: Counter({'left': 152, 'right': 134})\n",
            "breast-quad: Counter({'left_low': 110, 'left_up': 97, 'right_up': 33, 'right_low': 24, 'central': 21, '?': 1})\n",
            "irradiat: Counter({'no': 218, 'yes': 68})\n",
            "Class: Counter({'no-recurrence-events': 201, 'recurrence-events': 85})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All fields look to contain data that is catagorical in nature.**\n",
        "\n",
        "**Some contain data that appears erroneous:**\n",
        " \n",
        "*   **'tumor-size' and 'inv-nodes' appear to contain some data in a datetime format and some in string.**\n",
        "*   **'node-caps' and 'breast-quad' contain Question Marks.**\n",
        "\n",
        "**Need a way to address these erroneous data inputs.**\n",
        "\n"
      ],
      "metadata": {
        "id": "lALFUx2EEQF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look in more detail at the columns with datetime data.\n",
        "print(raw_data.iloc[:, 2].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IetVwnCr3XI",
        "outputId": "1e365ebb-c0a3-45c2-c35a-a9f3a986e1f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['15-19' '15-19' '35-39' '35-39' '30-34' '25-29' '40-44'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '0-4' '40-44' '25-29' '15-19'\n",
            " '30-34' '25-29' '25-29' '20-24' datetime.datetime(2014, 10, 1, 0, 0)\n",
            " '15-19' '40-44' '20-24' '20-24' '40-44' '15-19'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '15-19' '20-24'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) datetime.datetime(2014, 10, 1, 0, 0)\n",
            " '30-34' '15-19' '30-34' '25-29' '25-29' '20-24' '30-34' '15-19'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '45-49' '20-24'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '35-39' '35-39' '25-29' '20-24'\n",
            " '15-19' '30-34' datetime.datetime(2014, 10, 1, 0, 0) '35-39' '50-54'\n",
            " '40-44' '15-19' '30-34' '0-4' '40-44' '25-29' '25-29' '20-24' '35-39'\n",
            " '50-54' '0-4' '40-44' '30-34' '20-24' '30-34' '20-24' '15-19' '25-29'\n",
            " '15-19' '50-54' datetime.datetime(2014, 10, 1, 0, 0) '25-29' '25-29'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '30-34' '25-29'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '15-19' '25-29' '25-29' '30-34'\n",
            " '15-19' '25-29' '30-34' '15-19' '0-4' '35-39' '40-44' '25-29' '20-24'\n",
            " '30-34' '20-24' '30-34' '20-24' datetime.datetime(2014, 10, 1, 0, 0)\n",
            " '20-24' '45-49' '40-44' datetime.datetime(2014, 10, 1, 0, 0) '30-34'\n",
            " '35-39' '20-24' '15-19' '30-34' '20-24' '20-24' '30-34' '20-24' '25-29'\n",
            " '30-34' '20-24' '15-19' '30-34' '30-34' '40-44'\n",
            " datetime.datetime(2019, 9, 5, 0, 0) datetime.datetime(2014, 10, 1, 0, 0)\n",
            " '30-34' datetime.datetime(2014, 10, 1, 0, 0) '35-39' '20-24' '30-34'\n",
            " '25-29' '15-19' '35-39' datetime.datetime(2014, 10, 1, 0, 0) '30-34'\n",
            " '30-34' '25-29' '15-19' '15-19' '30-34' '35-39' '30-34' '25-29' '30-34'\n",
            " '15-19' '0-4' '0-4' '50-54' '30-34' '20-24' '25-29' '30-34' '20-24'\n",
            " '15-19' datetime.datetime(2014, 10, 1, 0, 0) '30-34'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '40-44' '30-34' '50-54' '15-19'\n",
            " '40-44' '25-29' datetime.datetime(2014, 10, 1, 0, 0)\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '30-34' '20-24'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '25-29' '25-29' '30-34' '50-54'\n",
            " '30-34' '20-24' '30-34' '25-29' '20-24' '20-24' '50-54' '20-24' '30-34'\n",
            " '25-29' '25-29' '40-44' '20-24' '20-24' '25-29' '25-29' '20-24' '40-44'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '35-39' '30-34'\n",
            " datetime.datetime(2019, 9, 5, 0, 0) '15-19' '30-34' '25-29'\n",
            " datetime.datetime(2019, 9, 5, 0, 0) '25-29' '25-29'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '35-39' '50-54' '25-29' '20-24'\n",
            " '30-34' '30-34' '15-19' '20-24' datetime.datetime(2019, 9, 5, 0, 0)\n",
            " '30-34' '30-34' '25-29' '25-29' '40-44' '25-29' '30-34' '30-34' '25-29'\n",
            " '25-29' '40-44' '20-24' '25-29' '20-24' '40-44' '25-29' '25-29' '45-49'\n",
            " '20-24' '25-29' '20-24' '20-24' '35-39' '20-24' '30-34' '25-29' '30-34'\n",
            " '25-29' '20-24' '20-24' datetime.datetime(2014, 10, 1, 0, 0) '15-19'\n",
            " '25-29' '20-24' '40-44' '15-19' '30-34' '30-34' '40-44' '30-34'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '40-44' '30-34' '30-34' '15-19'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '20-24'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '25-29' '30-34'\n",
            " datetime.datetime(2014, 10, 1, 0, 0) '30-34' '0-4' '25-29' '25-29'\n",
            " '40-44' '25-29' '30-34' '20-24' '20-24' '25-29' '30-34' '20-24' '30-34'\n",
            " '0-4' '20-24' '35-39' '30-34' '20-24' '25-29' '35-39' '20-24' '20-24'\n",
            " '35-39' '35-39' '25-29' '35-39' '30-34' '20-24' '15-19' '30-34' '25-29'\n",
            " '30-34' '15-19' '40-44']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at output data\n",
        "raw_data['Class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HePUlhNvfWF",
        "outputId": "480fe093-4d46-4895-f93d-5a256fb64b03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "no-recurrence-events    201\n",
              "recurrence-events        85\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only 2 possible outputs, thus needs converting to binary format for use in classifier models."
      ],
      "metadata": {
        "id": "bkV5bQKKwYHj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KMB3eKfC7goU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd10510-3b71-4bea-ee4f-8f5783c4a8e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True outputs: 29.72 %\n"
          ]
        }
      ],
      "source": [
        "# Check output balance\n",
        "out = raw_data.iloc[:, -1].values\n",
        "no_rows = len(raw_data)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "code_rows = le.fit_transform(out)\n",
        "print(\"True outputs: {:.2f} %\".format(sum(code_rows)/len(raw_data)*100))\n",
        "pos = sum(code_rows)\n",
        "neg = len(raw_data)-sum(code_rows)\n",
        "del out, no_rows, le, code_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clear imbalance between output data. Some degree of bias/weighting/sampling will be required to ensure that results accurately predict outcomes for both True and False outcomes."
      ],
      "metadata": {
        "id": "mMo9-0hTwirc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class weights\n",
        "weight_0 = (1 / neg) * ((pos + neg) / 2)\n",
        "weight_1 = (1 / pos) * ((pos + neg) / 2)\n",
        "class_weight = np.log([pos/neg])\n",
        "class_weight_dict = {0: weight_0, 1: weight_1}\n",
        "print(f\"Weight for 0: {weight_0}\")\n",
        "print(f\"Weight for 1: {weight_1}\")"
      ],
      "metadata": {
        "id": "CtCPaYUj8dcn",
        "outputId": "1aee68ce-4cba-4de1-8601-581b2a0e65aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight for 0: 0.7114427860696517\n",
            "Weight for 1: 1.6823529411764706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f02MTYgB7goW"
      },
      "outputs": [],
      "source": [
        "# Explain your key findings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data set is made up of 9 independent variables all of which appear catagorical in nature. Although stored as an integer, 'deg-malig' can be viewed as  catagorical data as it can only contain 3 discrete values.**\n",
        "\n",
        "**The inclusion of datetime data entries in both the 'tumor-size' and 'inv-nodes' fields appears to be caused by a formatting entry within Excel. For example, '10-14' being input erroneously as 10/14 thus Excel has interpreted (and converted) it to the datetime field 01/10/2014. A function will need to be written within the model to convert these back to correct format.**\n",
        "\n",
        "**How to deal with '?' entries in fields that are otherwise boolean poses an interesting dilemma. If these are infact meant to signify that the presence is unknown because no diagnostic work has been conducted, then this woiuld signify a valid dat entry. If it is however just an incomplete data entry then there is a risk its inclusion could skew the model results. Without knowing which it seems wisest to remove this data from the dataset. Removal of the entire field could well deprive the model of important information, thus just removing these specific entries (rows) appears the most sensible option, particularly noting that there are relatively few occurences.**\n",
        "\n",
        "**Data set is imbalanced, with dependent variable outputs only True in 30% of instances. The model applied will require this imbalance to be taken into account so as not to sacrifice results predicting this smaller class (surely the aim of cancer diagnosis) so as to achieve a high accuracy figure.**\n",
        "\n",
        "**Output variable will need converting into binary output for use with a binary classification model.**"
      ],
      "metadata": {
        "id": "V7OWyLcOrgWJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzZj8I8G7goX"
      },
      "source": [
        "###Create any data pre-processing that you will conduct on seen and unseen data.  Regardless of the model you use, this dataframe must contain only numeric features and have a strategy for any expected missing values. Any objects can that are needed to handle the test data that are dependent on the training data can be stored in the model class.  You are recommended to use sklearn Pipelines or similar functionality to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct date types in 'tumor-size' and 'inv-nodes' variables\n",
        "for i in range(0, len(raw_data)):\n",
        "    if type(raw_data['tumor-size'][i]) is not str:\n",
        "        if raw_data['tumor-size'][i].day == 1:\n",
        "            raw_data['tumor-size'][i] = str(raw_data['tumor-size'][i].month) +'-' + str(raw_data['tumor-size'][i].year-2000)\n",
        "        else:\n",
        "            raw_data['tumor-size'][i] = str(raw_data['tumor-size'][i].day) + '-' + str(raw_data['tumor-size'][i].month)\n",
        "    if type(raw_data['inv-nodes'][i]) is not str:\n",
        "        if raw_data['inv-nodes'][i].day == 1:\n",
        "            raw_data['inv-nodes'][i] = str(raw_data['inv-nodes'][i].month) + '-' + str(raw_data['inv-nodes'][i].year-2000)\n",
        "        else:\n",
        "            raw_data['inv-nodes'][i] = str(raw_data['inv-nodes'][i].day) + '-' + str(raw_data['inv-nodes'][i].month)        "
      ],
      "metadata": {
        "id": "GlZwiMllRpUB",
        "outputId": "fd3966f4-c0a1-40f4-d2ad-446ac63fb85f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all rows containing ? data\n",
        "indx = raw_data[raw_data.isin(['?'])].stack(dropna=True).unstack().index\n",
        "print(f\"indx: {indx}\")\n",
        "raw_data = raw_data.drop(index=indx)"
      ],
      "metadata": {
        "id": "ThhbSU5gPBYA",
        "outputId": "1f991dd1-f6c4-4b45-89d8-cb6eebe55b6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "indx: Int64Index([20, 31, 50, 54, 71, 92, 149, 240, 264], dtype='int64')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WIh9_0pp7goY"
      },
      "outputs": [],
      "source": [
        "# Split your data so that you can test the effectiveness of your model\n",
        "# Split the data into a Training set and a Test set\n",
        "dfs = np.split(raw_data, [len(raw_data.columns)-1], axis=1)\n",
        "X = dfs[0]\n",
        "y = dfs[1]\n",
        "\n",
        "# Handle categorical values and drop dummy variable\n",
        "# Remove non-categorical data\n",
        "dm = X.pop('deg-malig')\n",
        "# Encode the catagorical data (dummy variables)\n",
        "proc_X = pd.get_dummies(data=X, prefix_sep='_', drop_first=True)\n",
        "# Add back in non-categorical data\n",
        "proc_X.insert(0, 'deg-malig', dm)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(proc_X, y, test_size = 0.25, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AbOQACY77goY"
      },
      "outputs": [],
      "source": [
        "# Populate preprocess_training_data and preprocess_test_data to preprocess data.\n",
        "# You must process test and train separately so your model does not accidently gain information that a model wouldnt have in reality and therefore get better predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Xsq2f8747goZ"
      },
      "outputs": [],
      "source": [
        "#from keras.layers.advanced_activations import LeakyReLU\n",
        "class Module4_Model:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.metrics = [\n",
        "            keras.metrics.TruePositives(name='tp'),\n",
        "            keras.metrics.FalsePositives(name='fp'),\n",
        "            keras.metrics.TrueNegatives(name='tn'),\n",
        "            keras.metrics.FalseNegatives(name='fn'),\n",
        "            keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "            keras.metrics.Recall(name='recall'),\n",
        "            keras.metrics.Precision(name='precision'),\n",
        "            keras.metrics.AUC(name='prc', curve='PR'),\n",
        "            keras.metrics.SensitivityAtSpecificity(name='sen',specificity=0.8)\n",
        "        ]\n",
        "        self.EPOCHS = 500\n",
        "        self.BATCH = 100\n",
        "        self.THR = 0.3 #Bias towards positives to reduce incidents of FN at the expense of excess FPs.\n",
        "        self.stop_crit = keras.callbacks.EarlyStopping(\n",
        "            monitor='val_prc',\n",
        "            verbose=1,\n",
        "            patience=100,\n",
        "            mode='max',\n",
        "            restore_best_weights=True)\n",
        "\n",
        "    def preprocess_training_data(self, training_df):\n",
        "        \"\"\"\n",
        "        This function should process the training data and store any features\n",
        "        required in the class\n",
        "        \"\"\"         \n",
        "        # Apply feature scaling\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "        self.scalar = StandardScaler()\n",
        "        processed_df = self.scalar.fit_transform(training_df)\n",
        "        return processed_df\n",
        "\n",
        "    def preprocess_test_data(self, test_df):\n",
        "        \"\"\"\n",
        "        This function should process the test data and store any features\n",
        "        required in the class\n",
        "        \"\"\"\n",
        "        # Apply feature scaling\n",
        "        processed_df = self.scalar.transform(test_df)\n",
        "        return processed_df\n",
        "\n",
        "    def make_model(self,hp):\n",
        "        model = keras.Sequential()\n",
        "        output_bias = keras.initializers.Constant(class_weight)\n",
        "        # Tune the number of units in each layer\n",
        "        hp_units1 = hp.Int('units1',min_value=16,max_value=128,step=2)\n",
        "        hp_units2 = hp.Int('units2',min_value=16,max_value=64,step=2)\n",
        "        hp_units3 = hp.Int('units3',min_value=16,max_value=32,step=2)\n",
        "\n",
        "        model.add(Dense(hp_units1,\n",
        "                        activation=hp.Choice(\n",
        "                            name='dense_activation1',\n",
        "                            values=['tanh','relu','selu','leaky-relu'],\n",
        "                            default='selu'),\n",
        "                        input_shape=(x_train_processed.shape[-1],),\n",
        "                        kernel_initializer='lecun_normal'\n",
        "                        ))\n",
        "        model.add(Dense(hp_units2,\n",
        "                        activation=hp.Choice(\n",
        "                            name='dense_activation2',\n",
        "                            values=['tanh','relu','selu','leaky-relu'],\n",
        "                            default='leaky-relu'),\n",
        "                        kernel_initializer='lecun_normal'\n",
        "                        ))\n",
        "        model.add(Dense(hp_units3,\n",
        "                        activation=hp.Choice(\n",
        "                            name='dense_activation3',\n",
        "                            values=['tanh','relu','selu','leaky-relu'],\n",
        "                            default='leaky-relu'),\n",
        "                        kernel_initializer='lecun_normal'\n",
        "                        ))\n",
        "        model.add(Dense(1,kernel_initializer='normal',activation='sigmoid',bias_initializer=output_bias))\n",
        "        hp_learning_rate = hp.Choice('learning_rate',values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adamax(learning_rate=hp_learning_rate),\n",
        "            loss=keras.losses.BinaryCrossentropy(),\n",
        "            metrics=self.metrics)\n",
        "        \n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "F3LiNNCb7goa"
      },
      "outputs": [],
      "source": [
        "# Dont change\n",
        "my_model = Module4_Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZQD7WPdN7god"
      },
      "outputs": [],
      "source": [
        "# Dont change\n",
        "x_train_processed = my_model.preprocess_training_data(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the output data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb = LabelEncoder()\n",
        "y_train = pd.DataFrame(lb.fit_transform(y_train))\n",
        "y_test = pd.DataFrame(lb.transform(y_test))"
      ],
      "metadata": {
        "id": "xZNGF1UxWGU5",
        "outputId": "ff1c84df-5bd4-4e5c-c209-fceed76eb594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create model and train"
      ],
      "metadata": {
        "id": "deUEPqVyJUpX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XnLHgaXS7goe"
      },
      "outputs": [],
      "source": [
        "# Create a model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation,LeakyReLU\n",
        "from keras.losses import MeanSquaredLogarithmicError\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "msle = MeanSquaredLogarithmicError()\n",
        "get_custom_objects().update({'leaky-relu': Activation(LeakyReLU(alpha=0.3))})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilise HyperBand algorithm from keras tuner to construct model\n",
        "tuner = kt.Hyperband(\n",
        "    my_model.make_model,\n",
        "    objective=kt.Objective('sen', direction='max'),\n",
        "    max_epochs=50,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='keras_tuner',\n",
        ")\n",
        "tuner.search(x_train_processed,y_train,epochs=50,validation_split=0.2)"
      ],
      "metadata": {
        "id": "p1Ph9bZMV5G6",
        "outputId": "b8a3569b-8bcc-4a98-ee56-0cd418f1dc9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 90 Complete [00h 00m 08s]\n",
            "sen: 0.42592594027519226\n",
            "\n",
            "Best sen So Far: 1.0\n",
            "Total elapsed time: 00h 07m 35s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rQwUj4lk7goe"
      },
      "outputs": [],
      "source": [
        "# Dont change\n",
        "x_test_processed = my_model.preprocess_test_data(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for h_param in [f\"units{i}\" for i in range(1,4)] + ['learning_rate'] + [f\"dense_activation{i}\" for i in range(1,4)]:\n",
        "    print(h_param, tuner.get_best_hyperparameters()[0].get(h_param))    "
      ],
      "metadata": {
        "id": "HuWZxfcvYOI4",
        "outputId": "c9661bb5-ae30-4d19-c77f-d1c506a44873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "units1 90\n",
            "units2 44\n",
            "units3 24\n",
            "learning_rate 0.01\n",
            "dense_activation1 selu\n",
            "dense_activation2 relu\n",
            "dense_activation3 relu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.best_model = tuner.get_best_models()[0]\n",
        "my_model.best_model.build(x_train_processed.shape)\n",
        "my_model.best_model.summary()"
      ],
      "metadata": {
        "id": "Lku4uguEY1ar",
        "outputId": "9f4beeea-6a14-4cd8-9aae-bb879aee7b36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 90)                2880      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 44)                4004      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 24)                1080      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,989\n",
            "Trainable params: 7,989\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the best model (with class weights)\n",
        "my_model.best_model.fit(x_train_processed,\n",
        "                   y_train,\n",
        "                   batch_size=my_model.BATCH,\n",
        "                   epochs=my_model.EPOCHS,\n",
        "                   callbacks=[my_model.stop_crit],\n",
        "                   validation_split=0.2,\n",
        "                   class_weight=class_weight_dict)"
      ],
      "metadata": {
        "id": "DFAWtazI9LH4",
        "outputId": "feee150b-a8c1-4e1a-9ae2-97e35075f0a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "2/2 [==============================] - 4s 1s/step - loss: 0.1708 - tp: 49.0000 - fp: 2.0000 - tn: 143.0000 - fn: 13.0000 - accuracy: 0.9275 - recall: 0.7903 - precision: 0.9608 - prc: 0.9553 - sen: 0.9516 - val_loss: 0.9538 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2211 - val_sen: 0.0000e+00\n",
            "Epoch 2/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.1650 - tp: 49.0000 - fp: 2.0000 - tn: 109.0000 - fn: 5.0000 - accuracy: 0.9576 - recall: 0.9074 - precision: 0.9608 - prc: 0.9847 - sen: 1.0000 - val_loss: 0.9751 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2211 - val_sen: 0.0000e+00\n",
            "Epoch 3/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.1583 - tp: 49.0000 - fp: 2.0000 - tn: 109.0000 - fn: 5.0000 - accuracy: 0.9576 - recall: 0.9074 - precision: 0.9608 - prc: 0.9861 - sen: 1.0000 - val_loss: 1.0045 - val_tp: 3.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 5.0000 - val_accuracy: 0.6905 - val_recall: 0.3750 - val_precision: 0.2727 - val_prc: 0.2224 - val_sen: 0.0000e+00\n",
            "Epoch 4/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1518 - tp: 49.0000 - fp: 2.0000 - tn: 109.0000 - fn: 5.0000 - accuracy: 0.9576 - recall: 0.9074 - precision: 0.9608 - prc: 0.9875 - sen: 1.0000 - val_loss: 1.0309 - val_tp: 3.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 5.0000 - val_accuracy: 0.6905 - val_recall: 0.3750 - val_precision: 0.2727 - val_prc: 0.2247 - val_sen: 0.0000e+00\n",
            "Epoch 5/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1447 - tp: 49.0000 - fp: 3.0000 - tn: 108.0000 - fn: 5.0000 - accuracy: 0.9515 - recall: 0.9074 - precision: 0.9423 - prc: 0.9887 - sen: 1.0000 - val_loss: 1.0547 - val_tp: 3.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 5.0000 - val_accuracy: 0.6905 - val_recall: 0.3750 - val_precision: 0.2727 - val_prc: 0.2245 - val_sen: 0.0000e+00\n",
            "Epoch 6/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1399 - tp: 50.0000 - fp: 4.0000 - tn: 107.0000 - fn: 4.0000 - accuracy: 0.9515 - recall: 0.9259 - precision: 0.9259 - prc: 0.9882 - sen: 1.0000 - val_loss: 1.0771 - val_tp: 3.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 5.0000 - val_accuracy: 0.6905 - val_recall: 0.3750 - val_precision: 0.2727 - val_prc: 0.2247 - val_sen: 0.0000e+00\n",
            "Epoch 7/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1354 - tp: 52.0000 - fp: 5.0000 - tn: 106.0000 - fn: 2.0000 - accuracy: 0.9576 - recall: 0.9630 - precision: 0.9123 - prc: 0.9887 - sen: 1.0000 - val_loss: 1.0919 - val_tp: 4.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 4.0000 - val_accuracy: 0.7143 - val_recall: 0.5000 - val_precision: 0.3333 - val_prc: 0.2289 - val_sen: 0.0000e+00\n",
            "Epoch 8/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1315 - tp: 52.0000 - fp: 6.0000 - tn: 105.0000 - fn: 2.0000 - accuracy: 0.9515 - recall: 0.9630 - precision: 0.8966 - prc: 0.9888 - sen: 1.0000 - val_loss: 1.0988 - val_tp: 4.0000 - val_fp: 9.0000 - val_tn: 25.0000 - val_fn: 4.0000 - val_accuracy: 0.6905 - val_recall: 0.5000 - val_precision: 0.3077 - val_prc: 0.2267 - val_sen: 0.1250\n",
            "Epoch 9/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1274 - tp: 53.0000 - fp: 6.0000 - tn: 105.0000 - fn: 1.0000 - accuracy: 0.9576 - recall: 0.9815 - precision: 0.8983 - prc: 0.9897 - sen: 1.0000 - val_loss: 1.0992 - val_tp: 3.0000 - val_fp: 9.0000 - val_tn: 25.0000 - val_fn: 5.0000 - val_accuracy: 0.6667 - val_recall: 0.3750 - val_precision: 0.2500 - val_prc: 0.2277 - val_sen: 0.1250\n",
            "Epoch 10/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1233 - tp: 53.0000 - fp: 5.0000 - tn: 106.0000 - fn: 1.0000 - accuracy: 0.9636 - recall: 0.9815 - precision: 0.9138 - prc: 0.9903 - sen: 1.0000 - val_loss: 1.1035 - val_tp: 3.0000 - val_fp: 9.0000 - val_tn: 25.0000 - val_fn: 5.0000 - val_accuracy: 0.6667 - val_recall: 0.3750 - val_precision: 0.2500 - val_prc: 0.2284 - val_sen: 0.1250\n",
            "Epoch 11/500\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1198 - tp: 53.0000 - fp: 5.0000 - tn: 106.0000 - fn: 1.0000 - accuracy: 0.9636 - recall: 0.9815 - precision: 0.9138 - prc: 0.9905 - sen: 1.0000 - val_loss: 1.1051 - val_tp: 3.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 5.0000 - val_accuracy: 0.6905 - val_recall: 0.3750 - val_precision: 0.2727 - val_prc: 0.2309 - val_sen: 0.1250\n",
            "Epoch 12/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1166 - tp: 53.0000 - fp: 5.0000 - tn: 106.0000 - fn: 1.0000 - accuracy: 0.9636 - recall: 0.9815 - precision: 0.9138 - prc: 0.9910 - sen: 1.0000 - val_loss: 1.1099 - val_tp: 3.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 5.0000 - val_accuracy: 0.6905 - val_recall: 0.3750 - val_precision: 0.2727 - val_prc: 0.2357 - val_sen: 0.1250\n",
            "Epoch 13/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1134 - tp: 53.0000 - fp: 6.0000 - tn: 105.0000 - fn: 1.0000 - accuracy: 0.9576 - recall: 0.9815 - precision: 0.8983 - prc: 0.9905 - sen: 1.0000 - val_loss: 1.1229 - val_tp: 3.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 5.0000 - val_accuracy: 0.6905 - val_recall: 0.3750 - val_precision: 0.2727 - val_prc: 0.2313 - val_sen: 0.1250\n",
            "Epoch 14/500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.1101 - tp: 53.0000 - fp: 5.0000 - tn: 106.0000 - fn: 1.0000 - accuracy: 0.9636 - recall: 0.9815 - precision: 0.9138 - prc: 0.9918 - sen: 1.0000 - val_loss: 1.1343 - val_tp: 3.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 5.0000 - val_accuracy: 0.6905 - val_recall: 0.3750 - val_precision: 0.2727 - val_prc: 0.2307 - val_sen: 0.1250\n",
            "Epoch 15/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1068 - tp: 53.0000 - fp: 5.0000 - tn: 106.0000 - fn: 1.0000 - accuracy: 0.9636 - recall: 0.9815 - precision: 0.9138 - prc: 0.9922 - sen: 1.0000 - val_loss: 1.1456 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2298 - val_sen: 0.1250\n",
            "Epoch 16/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1042 - tp: 53.0000 - fp: 5.0000 - tn: 106.0000 - fn: 1.0000 - accuracy: 0.9636 - recall: 0.9815 - precision: 0.9138 - prc: 0.9925 - sen: 1.0000 - val_loss: 1.1564 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2352 - val_sen: 0.1250\n",
            "Epoch 17/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1016 - tp: 53.0000 - fp: 4.0000 - tn: 107.0000 - fn: 1.0000 - accuracy: 0.9697 - recall: 0.9815 - precision: 0.9298 - prc: 0.9928 - sen: 1.0000 - val_loss: 1.1666 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2359 - val_sen: 0.1250\n",
            "Epoch 18/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0992 - tp: 52.0000 - fp: 4.0000 - tn: 107.0000 - fn: 2.0000 - accuracy: 0.9636 - recall: 0.9630 - precision: 0.9286 - prc: 0.9927 - sen: 1.0000 - val_loss: 1.1794 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2353 - val_sen: 0.1250\n",
            "Epoch 19/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.0967 - tp: 52.0000 - fp: 4.0000 - tn: 107.0000 - fn: 2.0000 - accuracy: 0.9636 - recall: 0.9630 - precision: 0.9286 - prc: 0.9930 - sen: 1.0000 - val_loss: 1.1940 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2353 - val_sen: 0.1250\n",
            "Epoch 20/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0943 - tp: 52.0000 - fp: 4.0000 - tn: 107.0000 - fn: 2.0000 - accuracy: 0.9636 - recall: 0.9630 - precision: 0.9286 - prc: 0.9938 - sen: 1.0000 - val_loss: 1.2095 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2369 - val_sen: 0.1250\n",
            "Epoch 21/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.0920 - tp: 53.0000 - fp: 5.0000 - tn: 106.0000 - fn: 1.0000 - accuracy: 0.9636 - recall: 0.9815 - precision: 0.9138 - prc: 0.9945 - sen: 1.0000 - val_loss: 1.2310 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2353 - val_sen: 0.1250\n",
            "Epoch 22/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0897 - tp: 53.0000 - fp: 5.0000 - tn: 106.0000 - fn: 1.0000 - accuracy: 0.9636 - recall: 0.9815 - precision: 0.9138 - prc: 0.9955 - sen: 1.0000 - val_loss: 1.2491 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2363 - val_sen: 0.1250\n",
            "Epoch 23/500\n",
            "2/2 [==============================] - 0s 102ms/step - loss: 0.0877 - tp: 53.0000 - fp: 5.0000 - tn: 106.0000 - fn: 1.0000 - accuracy: 0.9636 - recall: 0.9815 - precision: 0.9138 - prc: 0.9954 - sen: 1.0000 - val_loss: 1.2695 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2354 - val_sen: 0.1250\n",
            "Epoch 24/500\n",
            "2/2 [==============================] - 0s 80ms/step - loss: 0.0859 - tp: 53.0000 - fp: 5.0000 - tn: 106.0000 - fn: 1.0000 - accuracy: 0.9636 - recall: 0.9815 - precision: 0.9138 - prc: 0.9952 - sen: 1.0000 - val_loss: 1.2855 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2354 - val_sen: 0.1250\n",
            "Epoch 25/500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0842 - tp: 53.0000 - fp: 5.0000 - tn: 106.0000 - fn: 1.0000 - accuracy: 0.9636 - recall: 0.9815 - precision: 0.9138 - prc: 0.9955 - sen: 1.0000 - val_loss: 1.2984 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2382 - val_sen: 0.1250\n",
            "Epoch 26/500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0823 - tp: 53.0000 - fp: 4.0000 - tn: 107.0000 - fn: 1.0000 - accuracy: 0.9697 - recall: 0.9815 - precision: 0.9298 - prc: 0.9961 - sen: 1.0000 - val_loss: 1.3108 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2366 - val_sen: 0.1250\n",
            "Epoch 27/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.0808 - tp: 53.0000 - fp: 4.0000 - tn: 107.0000 - fn: 1.0000 - accuracy: 0.9697 - recall: 0.9815 - precision: 0.9298 - prc: 0.9962 - sen: 1.0000 - val_loss: 1.3216 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2357 - val_sen: 0.1250\n",
            "Epoch 28/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.0790 - tp: 53.0000 - fp: 4.0000 - tn: 107.0000 - fn: 1.0000 - accuracy: 0.9697 - recall: 0.9815 - precision: 0.9298 - prc: 0.9964 - sen: 1.0000 - val_loss: 1.3271 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2350 - val_sen: 0.1250\n",
            "Epoch 29/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.0774 - tp: 53.0000 - fp: 4.0000 - tn: 107.0000 - fn: 1.0000 - accuracy: 0.9697 - recall: 0.9815 - precision: 0.9298 - prc: 0.9965 - sen: 1.0000 - val_loss: 1.3308 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2360 - val_sen: 0.1250\n",
            "Epoch 30/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.0757 - tp: 53.0000 - fp: 4.0000 - tn: 107.0000 - fn: 1.0000 - accuracy: 0.9697 - recall: 0.9815 - precision: 0.9298 - prc: 0.9961 - sen: 1.0000 - val_loss: 1.3361 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2373 - val_sen: 0.1250\n",
            "Epoch 31/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.0743 - tp: 53.0000 - fp: 4.0000 - tn: 107.0000 - fn: 1.0000 - accuracy: 0.9697 - recall: 0.9815 - precision: 0.9298 - prc: 0.9967 - sen: 1.0000 - val_loss: 1.3388 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2378 - val_sen: 0.1250\n",
            "Epoch 32/500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0725 - tp: 53.0000 - fp: 4.0000 - tn: 107.0000 - fn: 1.0000 - accuracy: 0.9697 - recall: 0.9815 - precision: 0.9298 - prc: 0.9966 - sen: 1.0000 - val_loss: 1.3458 - val_tp: 2.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 6.0000 - val_accuracy: 0.6905 - val_recall: 0.2500 - val_precision: 0.2222 - val_prc: 0.2357 - val_sen: 0.1250\n",
            "Epoch 33/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0711 - tp: 53.0000 - fp: 4.0000 - tn: 107.0000 - fn: 1.0000 - accuracy: 0.9697 - recall: 0.9815 - precision: 0.9298 - prc: 0.9966 - sen: 1.0000 - val_loss: 1.3554 - val_tp: 2.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 6.0000 - val_accuracy: 0.6905 - val_recall: 0.2500 - val_precision: 0.2222 - val_prc: 0.2350 - val_sen: 0.1250\n",
            "Epoch 34/500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0700 - tp: 53.0000 - fp: 4.0000 - tn: 107.0000 - fn: 1.0000 - accuracy: 0.9697 - recall: 0.9815 - precision: 0.9298 - prc: 0.9968 - sen: 1.0000 - val_loss: 1.3709 - val_tp: 2.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 6.0000 - val_accuracy: 0.6905 - val_recall: 0.2500 - val_precision: 0.2222 - val_prc: 0.2368 - val_sen: 0.1250\n",
            "Epoch 35/500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0687 - tp: 53.0000 - fp: 3.0000 - tn: 108.0000 - fn: 1.0000 - accuracy: 0.9758 - recall: 0.9815 - precision: 0.9464 - prc: 0.9966 - sen: 1.0000 - val_loss: 1.3777 - val_tp: 2.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 6.0000 - val_accuracy: 0.6905 - val_recall: 0.2500 - val_precision: 0.2222 - val_prc: 0.2368 - val_sen: 0.1250\n",
            "Epoch 36/500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.0672 - tp: 53.0000 - fp: 3.0000 - tn: 108.0000 - fn: 1.0000 - accuracy: 0.9758 - recall: 0.9815 - precision: 0.9464 - prc: 0.9970 - sen: 1.0000 - val_loss: 1.3901 - val_tp: 2.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 6.0000 - val_accuracy: 0.6905 - val_recall: 0.2500 - val_precision: 0.2222 - val_prc: 0.2368 - val_sen: 0.1250\n",
            "Epoch 37/500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0661 - tp: 53.0000 - fp: 3.0000 - tn: 108.0000 - fn: 1.0000 - accuracy: 0.9758 - recall: 0.9815 - precision: 0.9464 - prc: 0.9968 - sen: 1.0000 - val_loss: 1.4002 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2373 - val_sen: 0.1250\n",
            "Epoch 38/500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0649 - tp: 53.0000 - fp: 3.0000 - tn: 108.0000 - fn: 1.0000 - accuracy: 0.9758 - recall: 0.9815 - precision: 0.9464 - prc: 0.9966 - sen: 1.0000 - val_loss: 1.4083 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2344 - val_sen: 0.1250\n",
            "Epoch 39/500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0637 - tp: 53.0000 - fp: 3.0000 - tn: 108.0000 - fn: 1.0000 - accuracy: 0.9758 - recall: 0.9815 - precision: 0.9464 - prc: 0.9971 - sen: 1.0000 - val_loss: 1.4162 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2350 - val_sen: 0.1250\n",
            "Epoch 40/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.0626 - tp: 53.0000 - fp: 3.0000 - tn: 108.0000 - fn: 1.0000 - accuracy: 0.9758 - recall: 0.9815 - precision: 0.9464 - prc: 0.9967 - sen: 1.0000 - val_loss: 1.4247 - val_tp: 3.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 5.0000 - val_accuracy: 0.7143 - val_recall: 0.3750 - val_precision: 0.3000 - val_prc: 0.2339 - val_sen: 0.1250\n",
            "Epoch 41/500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0615 - tp: 53.0000 - fp: 3.0000 - tn: 108.0000 - fn: 1.0000 - accuracy: 0.9758 - recall: 0.9815 - precision: 0.9464 - prc: 0.9969 - sen: 1.0000 - val_loss: 1.4307 - val_tp: 2.0000 - val_fp: 7.0000 - val_tn: 27.0000 - val_fn: 6.0000 - val_accuracy: 0.6905 - val_recall: 0.2500 - val_precision: 0.2222 - val_prc: 0.2305 - val_sen: 0.1250\n",
            "Epoch 42/500\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.0601 - tp: 53.0000 - fp: 3.0000 - tn: 108.0000 - fn: 1.0000 - accuracy: 0.9758 - recall: 0.9815 - precision: 0.9464 - prc: 0.9973 - sen: 1.0000 - val_loss: 1.4386 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2294 - val_sen: 0.1250\n",
            "Epoch 43/500\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0594 - tp: 53.0000 - fp: 3.0000 - tn: 108.0000 - fn: 1.0000 - accuracy: 0.9758 - recall: 0.9815 - precision: 0.9464 - prc: 0.9969 - sen: 1.0000 - val_loss: 1.4455 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2324 - val_sen: 0.1250\n",
            "Epoch 44/500\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.0585 - tp: 53.0000 - fp: 3.0000 - tn: 108.0000 - fn: 1.0000 - accuracy: 0.9758 - recall: 0.9815 - precision: 0.9464 - prc: 0.9969 - sen: 1.0000 - val_loss: 1.4593 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2302 - val_sen: 0.1250\n",
            "Epoch 45/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.0572 - tp: 53.0000 - fp: 3.0000 - tn: 108.0000 - fn: 1.0000 - accuracy: 0.9758 - recall: 0.9815 - precision: 0.9464 - prc: 0.9969 - sen: 1.0000 - val_loss: 1.4645 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2475 - val_sen: 0.1250\n",
            "Epoch 46/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.0563 - tp: 53.0000 - fp: 2.0000 - tn: 109.0000 - fn: 1.0000 - accuracy: 0.9818 - recall: 0.9815 - precision: 0.9636 - prc: 0.9971 - sen: 1.0000 - val_loss: 1.4704 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2482 - val_sen: 0.1250\n",
            "Epoch 47/500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0558 - tp: 53.0000 - fp: 3.0000 - tn: 108.0000 - fn: 1.0000 - accuracy: 0.9758 - recall: 0.9815 - precision: 0.9464 - prc: 0.9971 - sen: 1.0000 - val_loss: 1.4755 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2475 - val_sen: 0.1250\n",
            "Epoch 48/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.0551 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9974 - sen: 1.0000 - val_loss: 1.4911 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2465 - val_sen: 0.1250\n",
            "Epoch 49/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.0537 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.5000 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2390 - val_sen: 0.1250\n",
            "Epoch 50/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.0532 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9974 - sen: 1.0000 - val_loss: 1.5095 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2339 - val_sen: 0.1250\n",
            "Epoch 51/500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0522 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.5131 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2339 - val_sen: 0.1250\n",
            "Epoch 52/500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0518 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9978 - sen: 1.0000 - val_loss: 1.5185 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2346 - val_sen: 0.1250\n",
            "Epoch 53/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.0505 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.5227 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2350 - val_sen: 0.1250\n",
            "Epoch 54/500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0503 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.5270 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2332 - val_sen: 0.1250\n",
            "Epoch 55/500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0498 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.5431 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2339 - val_sen: 0.1250\n",
            "Epoch 56/500\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.0486 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.5525 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2337 - val_sen: 0.1250\n",
            "Epoch 57/500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0478 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9985 - sen: 1.0000 - val_loss: 1.5549 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2347 - val_sen: 0.1250\n",
            "Epoch 58/500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0472 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9978 - sen: 1.0000 - val_loss: 1.5593 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2355 - val_sen: 0.1250\n",
            "Epoch 59/500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0469 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.5607 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2306 - val_sen: 0.1250\n",
            "Epoch 60/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.0464 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9975 - sen: 1.0000 - val_loss: 1.5701 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2302 - val_sen: 0.1250\n",
            "Epoch 61/500\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 0.0457 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.5729 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2286 - val_sen: 0.1250\n",
            "Epoch 62/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.0450 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.5835 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2302 - val_sen: 0.1250\n",
            "Epoch 63/500\n",
            "2/2 [==============================] - 0s 87ms/step - loss: 0.0443 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.5993 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2277 - val_sen: 0.1250\n",
            "Epoch 64/500\n",
            "2/2 [==============================] - 0s 88ms/step - loss: 0.0439 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.6127 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2285 - val_sen: 0.1250\n",
            "Epoch 65/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.0434 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.6239 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2285 - val_sen: 0.1250\n",
            "Epoch 66/500\n",
            "2/2 [==============================] - 0s 85ms/step - loss: 0.0431 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.6321 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2285 - val_sen: 0.1250\n",
            "Epoch 67/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.0436 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9976 - sen: 1.0000 - val_loss: 1.6361 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2294 - val_sen: 0.1250\n",
            "Epoch 68/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.0426 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.6475 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2251 - val_sen: 0.1250\n",
            "Epoch 69/500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0421 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.6523 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2249 - val_sen: 0.1250\n",
            "Epoch 70/500\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.0416 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.6521 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2244 - val_sen: 0.1250\n",
            "Epoch 71/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.0415 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.6474 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2292 - val_sen: 0.1250\n",
            "Epoch 72/500\n",
            "2/2 [==============================] - 0s 84ms/step - loss: 0.0423 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9978 - sen: 1.0000 - val_loss: 1.6465 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2276 - val_sen: 0.1250\n",
            "Epoch 73/500\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.0415 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.6644 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2234 - val_sen: 0.1250\n",
            "Epoch 74/500\n",
            "2/2 [==============================] - 0s 84ms/step - loss: 0.0407 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.6756 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2234 - val_sen: 0.1250\n",
            "Epoch 75/500\n",
            "2/2 [==============================] - 0s 87ms/step - loss: 0.0401 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.6916 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2225 - val_sen: 0.1250\n",
            "Epoch 76/500\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 0.0406 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.7123 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2202 - val_sen: 0.1250\n",
            "Epoch 77/500\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 0.0403 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9978 - sen: 1.0000 - val_loss: 1.7217 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2210 - val_sen: 0.1250\n",
            "Epoch 78/500\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 0.0398 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.7184 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2202 - val_sen: 0.1250\n",
            "Epoch 79/500\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 0.0392 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.7147 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2227 - val_sen: 0.1250\n",
            "Epoch 80/500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0388 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.7137 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2239 - val_sen: 0.1250\n",
            "Epoch 81/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.0391 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.7101 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2239 - val_sen: 0.1250\n",
            "Epoch 82/500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0385 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.7186 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2239 - val_sen: 0.1250\n",
            "Epoch 83/500\n",
            "2/2 [==============================] - 0s 92ms/step - loss: 0.0386 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9976 - sen: 1.0000 - val_loss: 1.7297 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2247 - val_sen: 0.1250\n",
            "Epoch 84/500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0381 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.7355 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2255 - val_sen: 0.1250\n",
            "Epoch 85/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.0380 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.7430 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2255 - val_sen: 0.1250\n",
            "Epoch 86/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.0377 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.7521 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2255 - val_sen: 0.1250\n",
            "Epoch 87/500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0375 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9979 - sen: 1.0000 - val_loss: 1.7628 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2227 - val_sen: 0.1250\n",
            "Epoch 88/500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0370 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9985 - sen: 1.0000 - val_loss: 1.7716 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2216 - val_sen: 0.1250\n",
            "Epoch 89/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0372 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.7784 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2176 - val_sen: 0.1250\n",
            "Epoch 90/500\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.0368 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.7897 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2176 - val_sen: 0.1250\n",
            "Epoch 91/500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0370 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.7957 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2176 - val_sen: 0.1250\n",
            "Epoch 92/500\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 0.0366 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.8065 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2198 - val_sen: 0.1250\n",
            "Epoch 93/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.0367 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.8121 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2205 - val_sen: 0.1250\n",
            "Epoch 94/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.0368 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9976 - sen: 1.0000 - val_loss: 1.8202 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2182 - val_sen: 0.0000e+00\n",
            "Epoch 95/500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0367 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9976 - sen: 1.0000 - val_loss: 1.8158 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2235 - val_sen: 0.1250\n",
            "Epoch 96/500\n",
            "2/2 [==============================] - 0s 124ms/step - loss: 0.0363 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.8175 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2243 - val_sen: 0.1250\n",
            "Epoch 97/500\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 0.0365 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9977 - sen: 1.0000 - val_loss: 1.8116 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2261 - val_sen: 0.1250\n",
            "Epoch 98/500\n",
            "2/2 [==============================] - 0s 87ms/step - loss: 0.0361 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.8132 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2243 - val_sen: 0.1250\n",
            "Epoch 99/500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0355 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.8229 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2243 - val_sen: 0.1250\n",
            "Epoch 100/500\n",
            "2/2 [==============================] - 0s 87ms/step - loss: 0.0355 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.8300 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2243 - val_sen: 0.1250\n",
            "Epoch 101/500\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 0.0357 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9978 - sen: 1.0000 - val_loss: 1.8415 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2221 - val_sen: 0.1250\n",
            "Epoch 102/500\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.0352 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.8570 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2181 - val_sen: 0.0000e+00\n",
            "Epoch 103/500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0356 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9978 - sen: 1.0000 - val_loss: 1.8651 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2181 - val_sen: 0.0000e+00\n",
            "Epoch 104/500\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 0.0354 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.8721 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2181 - val_sen: 0.0000e+00\n",
            "Epoch 105/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.0354 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.8775 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2181 - val_sen: 0.0000e+00\n",
            "Epoch 106/500\n",
            "2/2 [==============================] - 0s 126ms/step - loss: 0.0356 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.8838 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2173 - val_sen: 0.0000e+00\n",
            "Epoch 107/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.0357 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.8768 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2191 - val_sen: 0.0000e+00\n",
            "Epoch 108/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.0350 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.8752 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2178 - val_sen: 0.0000e+00\n",
            "Epoch 109/500\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.0354 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9977 - sen: 1.0000 - val_loss: 1.8676 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2210 - val_sen: 0.1250\n",
            "Epoch 110/500\n",
            "2/2 [==============================] - 0s 83ms/step - loss: 0.0344 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.8745 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2210 - val_sen: 0.1250\n",
            "Epoch 111/500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0346 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.8840 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2210 - val_sen: 0.1250\n",
            "Epoch 112/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.0345 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.8916 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2187 - val_sen: 0.0000e+00\n",
            "Epoch 113/500\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 0.0340 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.9045 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2183 - val_sen: 0.0000e+00\n",
            "Epoch 114/500\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.0340 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9985 - sen: 1.0000 - val_loss: 1.9173 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2195 - val_sen: 0.0000e+00\n",
            "Epoch 115/500\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 0.0345 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.9286 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2194 - val_sen: 0.0000e+00\n",
            "Epoch 116/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0347 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.9354 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2186 - val_sen: 0.0000e+00\n",
            "Epoch 117/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.0346 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.9389 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2164 - val_sen: 0.0000e+00\n",
            "Epoch 118/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.0347 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.9367 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2194 - val_sen: 0.0000e+00\n",
            "Epoch 119/500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0348 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9978 - sen: 1.0000 - val_loss: 1.9202 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2190 - val_sen: 0.0000e+00\n",
            "Epoch 120/500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0336 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.9147 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2192 - val_sen: 0.0000e+00\n",
            "Epoch 121/500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0334 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.9041 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2205 - val_sen: 0.1250\n",
            "Epoch 122/500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0337 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.8925 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2240 - val_sen: 0.1250\n",
            "Epoch 123/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 0.0354 - tp: 53.0000 - fp: 3.0000 - tn: 108.0000 - fn: 1.0000 - accuracy: 0.9758 - recall: 0.9815 - precision: 0.9464 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.8921 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2249 - val_sen: 0.1250\n",
            "Epoch 124/500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0342 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9985 - sen: 1.0000 - val_loss: 1.9168 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2200 - val_sen: 0.1250\n",
            "Epoch 125/500\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.0335 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.9424 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2196 - val_sen: 0.0000e+00\n",
            "Epoch 126/500\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.0335 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.9594 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2177 - val_sen: 0.0000e+00\n",
            "Epoch 127/500\n",
            "2/2 [==============================] - 0s 86ms/step - loss: 0.0342 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.9755 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2177 - val_sen: 0.0000e+00\n",
            "Epoch 128/500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0347 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9978 - sen: 1.0000 - val_loss: 1.9757 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2186 - val_sen: 0.0000e+00\n",
            "Epoch 129/500\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 0.0347 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.9778 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2186 - val_sen: 0.0000e+00\n",
            "Epoch 130/500\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 0.0339 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.9645 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2174 - val_sen: 0.0000e+00\n",
            "Epoch 131/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.0339 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.9504 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2192 - val_sen: 0.0000e+00\n",
            "Epoch 132/500\n",
            "2/2 [==============================] - 0s 86ms/step - loss: 0.0329 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.9473 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2214 - val_sen: 0.1250\n",
            "Epoch 133/500\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 0.0331 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.9492 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2214 - val_sen: 0.1250\n",
            "Epoch 134/500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0330 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.9480 - val_tp: 1.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 7.0000 - val_accuracy: 0.6429 - val_recall: 0.1250 - val_precision: 0.1111 - val_prc: 0.2231 - val_sen: 0.1250\n",
            "Epoch 135/500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0339 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9978 - sen: 1.0000 - val_loss: 1.9444 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2241 - val_sen: 0.1250\n",
            "Epoch 136/500\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.0335 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9978 - sen: 1.0000 - val_loss: 1.9560 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2213 - val_sen: 0.1250\n",
            "Epoch 137/500\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.0328 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.9753 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2199 - val_sen: 0.0000e+00\n",
            "Epoch 138/500\n",
            "2/2 [==============================] - 0s 83ms/step - loss: 0.0327 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9981 - sen: 1.0000 - val_loss: 1.9955 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2199 - val_sen: 0.0000e+00\n",
            "Epoch 139/500\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.0332 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 2.0147 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2190 - val_sen: 0.0000e+00\n",
            "Epoch 140/500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0341 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 2.0201 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2155 - val_sen: 0.0000e+00\n",
            "Epoch 141/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.0341 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9976 - sen: 1.0000 - val_loss: 2.0178 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2181 - val_sen: 0.0000e+00\n",
            "Epoch 142/500\n",
            "2/2 [==============================] - 0s 82ms/step - loss: 0.0339 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9978 - sen: 1.0000 - val_loss: 2.0076 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2207 - val_sen: 0.0000e+00\n",
            "Epoch 143/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.0330 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9979 - sen: 1.0000 - val_loss: 1.9990 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2195 - val_sen: 0.0000e+00\n",
            "Epoch 144/500\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 0.0326 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9983 - sen: 1.0000 - val_loss: 1.9901 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2176 - val_sen: 0.0000e+00\n",
            "Epoch 145/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.0334 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9977 - sen: 1.0000 - val_loss: 1.9738 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2207 - val_sen: 0.1250\n",
            "Epoch 146/500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0314 - tp: 33.0000 - fp: 1.0000 - tn: 66.0000 - fn: 0.0000e+00 - accuracy: 0.9900 - recall: 1.0000 - precision: 0.9706 - prc: 0.9977 - sen: 1.0000Restoring model weights from the end of the best epoch: 46.\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.0325 - tp: 54.0000 - fp: 3.0000 - tn: 108.0000 - fn: 0.0000e+00 - accuracy: 0.9818 - recall: 1.0000 - precision: 0.9474 - prc: 0.9980 - sen: 1.0000 - val_loss: 1.9746 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 26.0000 - val_fn: 6.0000 - val_accuracy: 0.6667 - val_recall: 0.2500 - val_precision: 0.2000 - val_prc: 0.2207 - val_sen: 0.1250\n",
            "Epoch 146: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5661198310>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualise the loss and accuracy for each epoch"
      ],
      "metadata": {
        "id": "G9DPmZDDXEnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for loss\n",
        "plt.plot(my_model.best_model.history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "84wnXBf0XDyz",
        "outputId": "830845f8-0a5d-4dce-aecb-2aa8382c46cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(my_model.best_model.history.history['accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c_bC8ZieW5fQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e32e3a5b-26f6-4eaf-fd0f-ee981c28384a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kwOiqNee7goj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd8db1ee-37d2-4d5a-a815-6a5172a6832e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 249ms/step\n"
          ]
        }
      ],
      "source": [
        "# use your model to make a prediction on unseen data\n",
        "y_pred = my_model.best_model.predict(x_test_processed,batch_size=my_model.BATCH)\n",
        "#convert values\n",
        "y_pred = (y_pred>my_model.THR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VGgleoej7gok",
        "outputId": "1e484539-0569-4b1e-e0c6-fad0dab1a925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 65.71 %\n",
            "Weighted ROC AUC accuracy: 64.91 %\n",
            "Confusion matrix:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATIAAAEGCAYAAADmLRl+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVm0lEQVR4nO3debAdZZ3G8e+TSyBhEYQECBBkXzIoAcNeIItA0NIERxGwFJURGEBcYCyGohSZcQYVZdABnLAUoCzCIAKKCciggBuESHYxUUACgZCEmISs997f/HH6wkm4uac7OUu/5z6fqi7O6XNO9y9J8dT7vv3224oIzMxSNqDVBZiZbSgHmZklz0FmZslzkJlZ8hxkZpa8jVpdQLUhW3fELsMHtroMK2D6K0NbXYIVsHrxQjqXv6ENOcaJx2wWCxZ25fru01NWToiI0RtyvjxKFWS7DB/IkxOGt7oMK2D/b57b6hKsgNm3f3eDj7FgYRdPTtg513c7hs0assEnzKFUQWZm5RdAN92tLmMNDjIzKyQIVke+rmWzOMjMrDC3yMwsaUHQVbJbGx1kZlZYNw4yM0tYAF0OMjNLnVtkZpa0AFZ7jMzMUhaEu5ZmlriArnLlmIPMzIqpzOwvFweZmRUkutig+87rzkFmZoVUBvsdZGaWsMo8MgeZmSWu2y0yM0uZW2RmlrxAdJVslXwHmZkV5q6lmSUtEKuio9VlrMFBZmaFVCbEumtpZonzYL+ZJS1CdIVbZGaWuG63yMwsZZXB/nJFR7mqMbPS82C/mbWFLs8jM7OUlXFmf7mqMbMkdMeAXFtfJA2S9KSkyZKmS/p6tn9XSX+QNFvSjyVtXKseB5mZFVK5aXxArq2GlcCxEbE/MBIYLelQ4JvAVRGxB/A6cGatAznIzKyQQKyOjlxbn8epWJq9HZhtARwL/G+2/xZgbK2aPEZmZoVEULcJsZI6gKeBPYBrgL8AiyKiM/vKHGDHWsdxkJlZQSoyIXaIpIlV78dFxLieNxHRBYyUtBVwL7DP+lTkIDOzQoJCLbL5ETGq5jEjFkl6FDgM2ErSRlmrbCfgpVq/9xiZmRVWj8F+SUOzlhiSBgPHAzOBR4GPZl87A7ivVj1ukZlZIYHqtbDiMOCWbJxsAHBXRPxM0gzgTkn/DvwRuLHWgRxkZlZI5XFwGx4dETEFOKCX/X8FDi5yLAeZmRXkB/SaWeICas7abzYHmZkV5haZmSUtQm6RmVnaKoP9foqSmSXNa/abWeIqg/0eIzOzxJVtYUUHmZkVUseZ/XXjIDOzwvzwETNLWgSs7naQmVnCKl1LB5mZJc4z+9vYqhXiwo/swepVA+jqhCM/+Hc+9S+vvPn5tZfuyIQ7t+a+2VNbWKVV+/pJj3LU7s+zcNlg/vGmUwH41ocf4l1bLwJgi0GrWLJiYz5+8ymtLLNU+t30C0mjgauBDuCGiLiikedrtYGbBN+6+y8M3qybztXw5bF7ctCxi9n3vcv48+TBLP17uWZDG9w3dW/umLQf3/jgI2/u+8r9J7z5+sJjfsvSlTWfRtbPlK9r2bBqssXSrgFOAkYAp0ka0ajzlYEEgzfrBqBztehaLSTo6oLr/20Hzrz05RZXaGubNGcHFi/fZB2fBifsM5tfzNyjqTWloDtbt7/W1iyNbJEdDMzOFklD0p3AGGBGA8/Zcl1dcP6Je/Py8xvzoU/PZ58Dl3HvDUM47ITFbLNdZ+0DWGkcuNNcFryxKX97fatWl1IqlauW5epdNLJ9uCPwYtX7Xh/rJOksSRMlTXxtQVcDy2mOjg647pfPctvTM3j2mU2Z+vvNePyBrRjz2ddaXZoVdNKIWYx3a+xteibE5tmapeUd3YgYFxGjImLU0G3KlfIbYvMtu9j/8KVM/s3mvPz8Jnzm8BF86uARrFw+gE8fvm+ry7MaOtTNcXs9x/g/Och605+6li8Bw6ve53qsU8oWLehgo40qIbZyuZj02Bacct487pw8/c3vjNnj3dz825ktrNLyOGSXOTy3YCvmLdm81aWUTn+7avkUsKekXakE2KnA6Q08X8stfHUgV35hZ7q7RXc3HPWhRRx6/OJWl2V9uOJDDzNq55fZavAKHjr3Vq574iDunbIvo/edzfiZe7a6vNIq21XLhgVZRHRKOh+YQGX6xU0RMb3Gz5K224gVXPvwn/v8jueQlcvFDxzf6/6vPnhskytJR4To7C9BBhARDwIPNvIcZtZ8/alraWZtqL+NkZlZm3KQmVnSvLCimbWFZs4Ry8NBZmaFRECnF1Y0s9S5a2lmSfMYmZm1hXCQmVnqyjbYX64ROzMrvQjqsoyPpOGSHpU0Q9J0SV/I9l8m6SVJz2TbB2rV5BaZmRUkuupz1bITuDAiJknaAnha0sPZZ1dFxJV5D+QgM7PC6jFGFhFzgbnZ6yWSZtLL4qt5uGtpZoX03GuZs2s5pGcF6Gw7q7djStoFOAD4Q7brfElTJN0k6Z21anKQmVkxURkny7MB83tWgM62cWsfTtLmwD3AFyNiMXAdsDswkkqL7Tu1SnLX0swKq9dVS0kDqYTYbRHxE4CIeLXq8+uBn9U6joPMzAqJOg32SxJwIzAzIr5btX9YNn4GcDIwrdaxHGRmVljWbdxQRwCfBKZKeibbdwmVZ+COpDIc9zxwdq0DOcjMrLA6XbV8AnrtoxZeVdpBZmaFVAbyyzWz30FmZoX5pnEzS16dxsjqxkFmZoUEotsLK5pZ6krWIHOQmVlBHuw3s7ZQsiaZg8zMCkumRSbp+/SRuxFxQUMqMrNSC6C7O5EgAyY2rQozS0cAqbTIIuKW6veSNo2IZY0vyczKrmzzyGpOBpF0mKQZwJ+y9/tLurbhlZlZeUXOrUnyzGr7L+BEYAFAREwGjmpkUWZWZiIi39Ysua5aRsSLlaWD3tTVmHLMLAkl61rmCbIXJR0ORLaa4xeAmY0ty8xKKyBKdtUyT9fyHOA8Kk83eZnKOtrnNbIoMys75dyao2aLLCLmA59oQi1mloqSdS3zXLXcTdIDkl6TNE/SfZJ2a0ZxZlZSCV61vB24CxgG7ADcDdzRyKLMrMR6JsTm2ZokT5BtGhE/jIjObPsRMKjRhZlZeRV4rmVT9HWv5dbZy19Iuhi4k0oWf5z1eDiAmbWRkl217Guw/2kqwdVTcfUjmQL410YVZWblppIN9vd1r+WuzSzEzBLR5IH8PHLN7Je0HzCCqrGxiLi1UUWZWZk1dyA/j5pBJulrwNFUguxB4CTgCcBBZtZflaxFlueq5UeB44BXIuIzwP7Alg2tyszKrTvn1iR5upbLI6JbUqekdwDzgOENrsvMyiqlhRWrTJS0FXA9lSuZS4HfNbQqMyu1ZK5a9oiIc7OXP5A0HnhHRExpbFlmVmqpBJmkA/v6LCImNaYkM7Ni+mqRfaePzwI4ts618Ocpm3LiDiPrfVhroG2PXN7qEqyAF5bVZwQ+ma5lRBzTzELMLBFBXW5RkjScyjSu7bKjjouIq7PbI38M7AI8D5wSEa/3daw80y/MzNZUn2V8OoELI2IEcChwnqQRwMXAIxGxJ/BI9r5PDjIzK0yRb+tLRMztGWuPiCVUltDfERgD9DyO8hZgbK16ct2iZGa2hvxjZEMkVT/se1xEjFv7S5J2AQ4A/gBsFxFzs49eodL17FOeW5REZanr3SLickk7A9tHxJM1/whm1p7yB9n8iBjV1xckbQ7cA3wxIhZXP7EtIkKqfWkhT9fyWuAw4LTs/RLgmhy/M7M2lLdbmefKZvZktnuA2yLiJ9nuVyUNyz4fRuVuoj7lCbJDIuI8YAVAdvVg4xy/M7N21a18Wx+y3t6NwMyI+G7VR/cDZ2SvzwDuq1VOnjGy1ZI6yBqTkobS1NtBzaxs6jSP7Ajgk8BUSc9k+y4BrgDuknQm8AJwSq0D5Qmy7wH3AttK+gaV1TAuXZ+qzaxN1CHIIuIJ1v3wy+OKHCvPvZa3SXo6O7CAsRHhJ42b9Vc5x7+aKc9Vy52BZcAD1fsi4m+NLMzMSiy1IAN+zlsPIRkE7Ao8C/xDA+sysxJTyUbJ83Qt3139PlsV49x1fN3MrOkKz+yPiEmSDmlEMWaWiNS6lpK+XPV2AHAg8HLDKjKzcktxsB/Youp1J5Uxs3saU46ZJSGlIMsmwm4RERc1qR4zS0EqQSZpo4jolHREMwsys3ITaV21fJLKeNgzku4H7gbe6Pmw6gZPM+tPEh0jGwQsoLJGf898sgAcZGb9VUJBtm12xXIabwVYj5L9McysqUqWAH0FWQewOb3f1FmyP4aZNVNKXcu5EXF50yoxs3QkFGQb/rwnM2s/kdZVy0LrAZlZP5JKiywiFjazEDNLR0pjZGZmvXOQmVnS8j1FvKkcZGZWiHDX0szagIPMzNLnIDOz5DnIzCxpia5+YWa2JgeZmaUupVuUzMx65a6lmaXNE2LNrC04yMwsZZ7Zb2ZtQd3lSrIBrS7AzBITBbYaJN0kaZ6kaVX7LpP0kqRnsu0DtY7jIDOzwhT5thxuBkb3sv+qiBiZbQ/WOoiDzMyKq1OLLCIeAzZ4EVcHmZkVVqBFNkTSxKrtrJynOF/SlKzr+c5aX3aQmVlx+Vtk8yNiVNU2LsfRrwN2B0YCc4Hv1PqBr1qaWTENfopSRLza81rS9cDPav3GLTIzK6RnHlmdBvvffnxpWNXbk4Fp6/puD7fIzKy4qM88Mkl3AEdTGUubA3wNOFrSSCqd0+eBs2sdx0FmZoXVa2Z/RJzWy+4bix7HQdZAO+2+gkt+8MKb77ffeRU//Pb23HvD0BZWZdUuPOcJDjlwDosWD+Ksi8YC8LlPPMWh732Rzs4OXn51C6687gjeWLZJiystkRLeNN6wMbLeZuz2N3P+Mohzj9+bc4/fm/NP3IuVywfwm19s2eqyrMpDv96DS/7z+DX2TZq6A5+7aCxnf2UML819B6eNndqi6spL3fm2ZmnkYP/N9D5jt18aeeRS5r6wMfNe2rjVpViVqTO3Z8nSNf9Nnp6yI93dlf81Zs4aypBtlrWitFLrN0FWrxm77eLoMa/zq5/WnNdnJXPiMbN46o87trqMcgkqg/15tiZp+fQLSWf1zPpdzcpWl9MQGw3s5tATFvPYA+5WpuT0kyfT1TWAR57YrdWllE4jp1+sj5YHWUSM65n1O5D2HFA96NglzJ46mEXzB7a6FMvphPfN4pAD53DF94+iMnPK1lCney3rxVctm+DosYvcrUzIqP3ncMqHp3HhZSexcpX/F1mbF1bshzYZ3MWBRy7h6q/s1OpSrBeXXPBr3jPiFbbcYgW3X3sXt949klPHTmXgRl1889IJQGXA/+obDm9xpSUSUbqFFRsWZL3N2I2IwhPdUrdyeQcf22+/Vpdh6/Af33vf2/aNf3SvFlSSmHLlWOOCbB0zds2sDbhraWZpC6C/dC3NrI2VK8ccZGZWnLuWZpa8fnPV0szaVAlXv3CQmVkhlQmx5UoyB5mZFdfElS3ycJCZWWFukZlZ2jxGZmbp60f3WppZG3PX0syS1uAH9K4PB5mZFecWmZklr1w55iAzs+LUXa6+pYPMzIoJPCHWzNImwhNizawNOMjMLHkOMjNLmsfIzKwdlO2qZcufNG5mqYlK1zLPVoOkmyTNkzStat/Wkh6WNCv7b82nWzvIzKyYoG5BBtwMjF5r38XAIxGxJ/BI9r5PDjIzK64751ZDRDwGLFxr9xjgluz1LcDYWsfxGJmZFdbgeWTbRcTc7PUrwHa1fuAgM7Pi8gfZEEkTq96Pi4hx+U8TIdV++JyDzMyKiYCu3Fct50fEqIJneFXSsIiYK2kYMK/WDzxGZmbF1W+wvzf3A2dkr88A7qv1AweZmRVXv+kXdwC/A/aWNEfSmcAVwPGSZgHvz973yV1LMysmgDqt2R8Rp63jo+OKHMdBZmYFBUS5ZvY7yMysmKDIYH9TOMjMrDivfmFmyXOQmVnaNmhqRUM4yMysmABKtoyPg8zMinOLzMzSVugWpaZwkJlZMQHheWRmlrw6zeyvFweZmRXnMTIzS1qEr1qaWRtwi8zM0hZEV1eri1iDg8zMiqnjMj714iAzs+I8/cLMUhZAuEVmZkkLL6xoZm2gbIP9ihJdRpX0GvBCq+togCHA/FYXYYW067/ZuyJi6IYcQNJ4Kn8/ecyPiNEbcr48ShVk7UrSxPV4tp+1kP/N0uLHwZlZ8hxkZpY8B1lzjGt1AVaY/80S4jEyM0ueW2RmljwHmZklz0HWQJJGS3pW0mxJF7e6HqtN0k2S5kma1upaLD8HWYNI6gCuAU4CRgCnSRrR2qosh5uBhk/gtPpykDXOwcDsiPhrRKwC7gTGtLgmqyEiHgMWtroOK8ZB1jg7Ai9WvZ+T7TOzOnOQmVnyHGSN8xIwvOr9Ttk+M6szB1njPAXsKWlXSRsDpwL3t7gms7bkIGuQiOgEzgcmADOBuyJiemurslok3QH8Dthb0hxJZ7a6JqvNtyiZWfLcIjOz5DnIzCx5DjIzS56DzMyS5yAzs+Q5yBIiqUvSM5KmSbpb0qYbcKybJX00e31DXze0Szpa0uHrcY7nJb3taTvr2r/Wd5YWPNdlki4qWqO1BwdZWpZHxMiI2A9YBZxT/aGk9XpOaUT8U0TM6OMrRwOFg8ysWRxk6Xoc2CNrLT0u6X5ghqQOSd+W9JSkKZLOBlDFf2fro/0S2LbnQJJ+JWlU9nq0pEmSJkt6RNIuVALzS1lr8EhJQyXdk53jKUlHZL/dRtJDkqZLugFQrT+EpJ9Kejr7zVlrfXZVtv8RSUOzfbtLGp/95nFJ+9TjL9PS5ieNJyhreZ0EjM92HQjsFxHPZWHw94g4SNImwG8kPQQcAOxNZW207YAZwE1rHXcocD1wVHasrSNioaQfAEsj4srse7cDV0XEE5J2pnL3wr7A14AnIuJySR8E8syK/2x2jsHAU5LuiYgFwGbAxIj4kqSvZsc+n8pDQc6JiFmSDgGuBY5dj79GayMOsrQMlvRM9vpx4EYqXb4nI+K5bP8JwHt6xr+ALYE9gaOAOyKiC3hZ0v/1cvxDgcd6jhUR61qX6/3ACOnNBtc7JG2eneMj2W9/Lun1HH+mCySdnL0entW6AOgGfpzt/xHwk+wchwN3V517kxznsDbnIEvL8ogYWb0j+x/6jepdwOcjYsJa3/tAHesYABwaESt6qSU3SUdTCcXDImKZpF8Bg9bx9cjOu2jtvwMzj5G1nwnAP0saCCBpL0mbAY8BH8/G0IYBx/Ty298DR0naNfvt1tn+JcAWVd97CPh8zxtJPcHyGHB6tu8k4J01at0SeD0LsX2otAh7DAB6WpWnU+myLgaek/Sx7ByStH+Nc1g/4CBrPzdQGf+alD1A43+otLzvBWZln91KZYWHNUTEa8BZVLpxk3mra/cAcHLPYD9wATAqu5gwg7eunn6dShBOp9LF/FuNWscDG0maCVxBJUh7vAEcnP0ZjgUuz/Z/Ajgzq286Xj7c8OoXZtYG3CIzs+Q5yMwseQ4yM0ueg8zMkucgM7PkOcjMLHkOMjNL3v8DruBU4IQ1YSIAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Asssess the accuracy of your model and explain your key findings\n",
        "# Generate confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {:.2f} %\".format(score*100))\n",
        "print(\"Weighted ROC AUC accuracy: {:.2f} %\".format(roc_auc_score(y_test, y_pred, average='weighted')*100))\n",
        "print(\"Confusion matrix:\")\n",
        "disp = ConfusionMatrixDisplay(cm)\n",
        "disp.plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model is approx 65-70% accurate at predicting whether cancer recurrence will occur.**\n",
        "\n",
        "**Crucially, the proportion of False Negatives is low (<=10%). In cancer diagnosis these are the outcomes that we want to minimise. False Positives, whilst undesirable, will likely lead to further diagnostic testing before it is realised that cancer is not present.**"
      ],
      "metadata": {
        "id": "UMNN4P_E07L6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDU85k7J7gok"
      },
      "source": [
        "### Unit tests:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq2QRpri7gol"
      },
      "source": [
        "###Checking training and test data for null values. This will work for both pd dataframes and np arrays, and ensures no null values exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oUeh38w_7gol"
      },
      "outputs": [],
      "source": [
        "def test_no_nulls(data):\n",
        "    \"\"\" Assert no null values within pd dataframe or np array \"\"\"\n",
        "    \n",
        "    # if data is numpy array, handle accordingly\n",
        "    if isinstance(data, (np.ndarray)):\n",
        "        assert not np.isnan(np.min(data))\n",
        "    \n",
        "    # if not np array, assume data is pandas dataframe\n",
        "    else:\n",
        "        assert data.isna().sum().sum() == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BV9Z1F3i7gom"
      },
      "outputs": [],
      "source": [
        "# run null data unit test on both training and test data\n",
        "test_no_nulls(x_train_processed)\n",
        "test_no_nulls(x_test_processed)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of KSVC.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}